{"metadata":{"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_datasets as tfds\nimport tensorflow.keras as keras\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport os\nimport zipfile\nimport urllib.request\nimport random\nfrom IPython import display\n\n# set a random seed\nnp.random.seed(51)\n\n# parameters for building the model and training\nBATCH_SIZE=64\nLATENT_DIM=512\nIMAGE_SIZE=64\nchannelThree = True\nonesideSmooth = False\nnoisyLabels = False\n\n# Data Preparation Utilities\n\ndef get_dataset_slice_paths(image_dir):\n  '''returns a list of paths to the image files'''\n  image_file_list = os.listdir(image_dir)\n  image_paths = [os.path.join(image_dir, fname) for fname in image_file_list]\n\n  return image_paths\n\n\ndef map_image(image_filename):\n  '''preprocesses the images'''\n  img_raw = tf.io.read_file(image_filename)\n  image = tf.image.decode_png(img_raw, channels=3)\n\n  image = tf.cast(image, dtype=tf.float32)\n  image = tf.image.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n  image = image / 255.0  \n  image = tf.reshape(image, shape=(IMAGE_SIZE, IMAGE_SIZE, 3,))\n\n  return image\n\ndef get_data(filepath):    \n    # get the list containing the image paths\n    paths = get_dataset_slice_paths(filepath)\n\n    # shuffle the paths\n    random.shuffle(paths)\n\n    # split the paths list into to training (80%) and validation sets(20%).\n    paths_len = len(paths)\n    train_paths_len = int(paths_len * 0.8)\n\n    train_paths = paths[:train_paths_len]\n    val_paths = paths[train_paths_len:]\n\n    # load the training image paths into tensors, create batches and shuffle\n    training_dataset = tf.data.Dataset.from_tensor_slices((train_paths))\n    training_dataset = training_dataset.map(map_image)\n    training_dataset = training_dataset.shuffle(1000).batch(BATCH_SIZE)\n\n    # load the validation image paths into tensors and create batches\n    validation_dataset = tf.data.Dataset.from_tensor_slices((val_paths))\n    validation_dataset = validation_dataset.map(map_image)\n    validation_dataset = validation_dataset.batch(BATCH_SIZE)\n\n\n    print(f'number of batches in the training set: {len(training_dataset)}')\n    print(f'number of batches in the validation set: {len(validation_dataset)}')\n    \n    return training_dataset, validation_dataset\n\ndef display_faces(dataset, size=9):\n  '''Takes a sample from a dataset batch and plots it in a grid.'''\n  dataset = dataset.unbatch().take(size)\n  n_cols = 3\n  n_rows = size//n_cols + 1\n  plt.figure(figsize=(5, 5))\n  i = 0\n  for image in dataset:\n    i += 1\n    disp_img = np.reshape(image, (64,64,3))\n    plt.subplot(n_rows, n_cols, i)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(disp_img)\n\n\ndef display_one_row(disp_images, offset, shape=(28, 28)):\n  '''Displays a row of images.'''\n  for idx, image in enumerate(disp_images):\n    plt.subplot(3, 10, offset + idx + 1)\n    plt.xticks([])\n    plt.yticks([])\n    image = np.reshape(image, shape)\n    plt.imshow(image)\n\n\ndef display_results(disp_input_images, disp_predicted):\n  '''Displays input and predicted images.'''\n  plt.figure(figsize=(15, 5))\n  display_one_row(disp_input_images, 0, shape=(IMAGE_SIZE,IMAGE_SIZE,3))\n  display_one_row(disp_predicted, 20, shape=(IMAGE_SIZE,IMAGE_SIZE,3))\n\nclass VAE_Sampling(tf.keras.layers.Layer):\n  def call(self, inputs):\n    \"\"\"Generates a random sample and combines with the encoder output\n    \n    Args:\n      inputs -- output tensor from the encoder\n\n    Returns:\n      `inputs` tensors combined with a random sample\n    \"\"\"\n    mu, sigma = inputs\n\n    # get the size and dimensions of the batch\n    batch = tf.shape(mu)[0]\n    dim = tf.shape(mu)[1]\n\n    # generate a random tensor\n    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n    z = mu + tf.exp(0.5 * sigma) * epsilon\n    return  z\n\ndef VAE_encoder_layers(inputs, latent_dim):\n  \"\"\"Defines the encoder's layers.\n  Args:\n    inputs -- batch from the dataset\n    latent_dim -- dimensionality of the latent space\n\n  Returns:\n    mu -- learned mean\n    sigma -- learned standard deviation\n    batch_3.shape -- shape of the features before flattening\n  \"\"\"\n  x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding=\"same\", activation='relu', name=\"encode_conv1\")(inputs)\n  x = tf.keras.layers.BatchNormalization()(x)\n  x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='same', activation='relu', name=\"encode_conv2\")(x)\n  x = tf.keras.layers.BatchNormalization()(x)\n  x = tf.keras.layers.Conv2D(filters=128, kernel_size=3, strides=2, padding='same', activation='relu', name=\"encode_conv3\")(x)  \n\n  # assign to a different variable so you can extract the shape later\n  batch_3 = tf.keras.layers.BatchNormalization()(x)\n\n  # flatten the features and feed into the Dense network\n  x = tf.keras.layers.Flatten(name=\"encode_flatten\")(batch_3)\n\n  # we arbitrarily used 20 units here but feel free to change and see what results you get\n  x = tf.keras.layers.Dense(1024, activation='relu', name=\"encode_dense\")(x)\n  x = tf.keras.layers.BatchNormalization()(x)\n\n  # add output Dense networks for mu and sigma, units equal to the declared latent_dim.\n  mu = tf.keras.layers.Dense(latent_dim, name='latent_mu')(x)\n  sigma = tf.keras.layers.Dense(latent_dim, name ='latent_sigma')(x)\n  \n  return mu, sigma, batch_3.shape\n\ndef VAE_encoder_model(latent_dim, input_shape):\n  \"\"\"Defines the encoder model with the Sampling layer\n  Args:\n    latent_dim -- dimensionality of the latent space\n    input_shape -- shape of the dataset batch\n\n  Returns:\n    model -- the encoder model\n    conv_shape -- shape of the features before flattening\n  \"\"\"\n  inputs = tf.keras.layers.Input(shape=input_shape)\n\n  # get the output of the encoder_layers() function\n  mu, sigma, conv_shape = VAE_encoder_layers(inputs, latent_dim=LATENT_DIM)\n\n  # feed mu and sigma to the Sampling layer\n  z = VAE_Sampling()((mu, sigma))\n\n  # build the whole encoder model\n  model = tf.keras.Model(inputs, outputs=[mu, sigma, z])\n  model.summary()\n  return model, conv_shape\n\ndef AE_encoder_layers(inputs, latent_dim):\n  \"\"\"Defines the encoder's layers.\n  Args:\n    inputs -- batch from the dataset\n    latent_dim -- dimensionality of the latent space\n\n  Returns:\n    x -- final model\n    batch_3.shape -- shape of the features before flattening\n  \"\"\"\n  x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding=\"same\", activation='relu', name=\"encode_conv1\")(inputs)\n  x = tf.keras.layers.BatchNormalization()(x)\n  x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='same', activation='relu', name=\"encode_conv2\")(x)\n  x = tf.keras.layers.BatchNormalization()(x)\n  x = tf.keras.layers.Conv2D(filters=128, kernel_size=3, strides=2, padding='same', activation='relu', name=\"encode_conv3\")(x)  \n\n  # assign to a different variable so you can extract the shape later\n  batch_3 = tf.keras.layers.BatchNormalization()(x)\n\n  # flatten the features and feed into the Dense network\n  x = tf.keras.layers.Flatten(name=\"encode_flatten\")(batch_3)\n\n  # we arbitrarily used 20 units here but feel free to change and see what results you get\n  x = tf.keras.layers.Dense(512, activation='relu', name=\"encode_dense\")(x)\n  x = tf.keras.layers.BatchNormalization()(x)\n\n  return x, batch_3.shape\n\ndef AE_encoder_model(latent_dim, input_shape):\n  \"\"\"Defines the encoder model\n  Args:\n    latent_dim -- dimensionality of the latent space\n    input_shape -- shape of the dataset batch\n\n  Returns:\n    model -- the encoder model\n    conv_shape -- shape of the features before flattening\n  \"\"\"\n  inputs = tf.keras.layers.Input(shape=input_shape)\n\n  # get the output of the encoder_layers() function\n  x, conv_shape = AE_encoder_layers(inputs, latent_dim=LATENT_DIM)\n  print(conv_shape)\n\n  # build the whole encoder model\n  model = tf.keras.Model(inputs, outputs=[x])\n  model.summary()\n  return model, conv_shape\n\ndef decoder_layers(inputs, conv_shape):\n  \"\"\"Defines the decoder layers.\n  Args:\n    inputs -- output of the encoder \n    conv_shape -- shape of the features before flattening\n\n  Returns:\n    tensor containing the decoded output\n  \"\"\"\n  # feed to a Dense network with units computed from the conv_shape dimensions\n  units = conv_shape[1] * conv_shape[2] * conv_shape[3]\n  x = tf.keras.layers.Dense(units, activation = 'relu', name=\"decode_dense1\")(inputs)\n  x = tf.keras.layers.BatchNormalization()(x)\n  \n  # reshape output using the conv_shape dimensions\n  x = tf.keras.layers.Reshape((conv_shape[1], conv_shape[2], conv_shape[3]), name=\"decode_reshape\")(x)\n\n  # upsample the features back to the original dimensions\n  x = tf.keras.layers.Conv2DTranspose(filters=128, kernel_size=3, strides=2, padding='same', activation='relu', name=\"decode_conv2d_1\")(x)\n  x = tf.keras.layers.BatchNormalization()(x)\n  x = tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same', activation='relu', name=\"decode_conv2d_2\")(x)\n  x = tf.keras.layers.BatchNormalization()(x)\n  x = tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3, strides=2, padding='same', activation='relu', name=\"decode_conv2d_3\")(x)\n  x = tf.keras.layers.BatchNormalization()(x)\n  x = tf.keras.layers.Conv2DTranspose(filters=3, kernel_size=3, strides=1, padding='same', activation='sigmoid', name=\"decode_final\")(x)\n  \n  return x\n\ndef decoder_model(latent_dim, conv_shape):\n  \"\"\"Defines the decoder model.\n  Args:\n    latent_dim -- dimensionality of the latent space\n    conv_shape -- shape of the features before flattening\n\n  Returns:\n    model -- the decoder model\n  \"\"\"\n  # set the inputs to the shape of the latent space\n  inputs = tf.keras.layers.Input(shape=(latent_dim,))\n\n  # get the output of the decoder layers\n  outputs = decoder_layers(inputs, conv_shape)\n\n  # declare the inputs and outputs of the model\n  model = tf.keras.Model(inputs, outputs)\n  model.summary()\n  return model\n\ndef kl_reconstruction_loss(inputs, outputs, mu, sigma):\n  \"\"\" Computes the Kullback-Leibler Divergence (KLD)\n  Args:\n    inputs -- batch from the dataset\n    outputs -- output of the Sampling layer\n    mu -- mean\n    sigma -- standard deviation\n\n  Returns:\n    KLD loss\n  \"\"\"\n  kl_loss = 1 + sigma - tf.square(mu) - tf.math.exp(sigma)\n  return tf.reduce_mean(kl_loss) * -0.5\n\ndef vae_model(encoder, decoder, input_shape):\n  \"\"\"Defines the VAE model\n  Args:\n    encoder -- the encoder model\n    decoder -- the decoder model\n    input_shape -- shape of the dataset batch\n\n  Returns:\n    the complete VAE model\n  \"\"\"\n  inputs = tf.keras.layers.Input(shape=input_shape)\n  print(inputs)\n  # get mu, sigma, and z from the encoder output\n  mu, sigma, z = encoder(inputs)\n  \n  # get reconstructed output from the decoder\n  reconstructed = decoder(z)\n\n  # define the inputs and outputs of the VAE\n  model = tf.keras.Model(inputs=inputs, outputs=reconstructed)\n\n  # add the KL loss\n  loss = kl_reconstruction_loss(inputs, z, mu, sigma)\n  model.add_loss(loss)\n  \n  return model\n\ndef get_VAE_models(input_shape, latent_dim):\n  \"\"\"Returns the encoder, decoder, and vae models\"\"\"\n  encoder, conv_shape = VAE_encoder_model(latent_dim=latent_dim, input_shape=input_shape)\n  decoder = decoder_model(latent_dim=latent_dim, conv_shape=conv_shape)\n  vae = vae_model(encoder, decoder, input_shape=input_shape)\n  return encoder, decoder, vae\n\ndef ae_model(encoder, decoder, input_shape):\n  \"\"\"Defines the AE model\n  Args:\n    encoder -- the encoder model\n    decoder -- the decoder model\n    input_shape -- shape of the dataset batch\n\n  Returns:\n    the complete VAE model\n  \"\"\"\n  inputs = tf.keras.layers.Input(shape=input_shape)\n  print(inputs)\n  # z from the encoder output\n  z = encoder(inputs)\n  \n  # get reconstructed output from the decoder\n  reconstructed = decoder(z)\n\n  # define the inputs and outputs of the VAE\n  model = tf.keras.Model(inputs=inputs, outputs=reconstructed)\n  model.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredError())\n  \n\n  return model\n\ndef get_AE_models(input_shape, latent_dim):\n  \"\"\"Returns the encoder, decoder, and ae models\"\"\"\n  encoder, conv_shape = AE_encoder_model(latent_dim=latent_dim, input_shape=input_shape)\n  decoder = decoder_model(latent_dim=latent_dim, conv_shape=conv_shape)\n  ae = ae_model(encoder, decoder, input_shape=input_shape)#Makes the complete model\n  return encoder, decoder, ae\n\n\ndef model_configs():    \n    optimizer = tf.keras.optimizers.Adam()\n    loss_metric = tf.keras.metrics.Mean()\n    mse_loss = tf.keras.losses.MeanSquaredError()\n    bce_loss = tf.keras.losses.BinaryCrossentropy()\n    \n    return optimizer, loss_metric, mse_loss, bce_loss\n    \ndef generate_and_save_images(model, epoch, step, test_input):\n  \"\"\"Helper function to plot our 16 images\n\n  Args:\n\n  model -- the decoder model\n  epoch -- current epoch number during training\n  step -- current step number during training\n  test_input -- random tensor with shape (16, LATENT_DIM)\n  \"\"\"\n  predictions = model.predict(test_input)\n\n  fig = plt.figure(figsize=(4,4))\n\n  for i in range(predictions.shape[0]):\n      plt.subplot(4, 4, i+1)\n      img = predictions[i, :, :, :] * 255\n      img = img.astype('int32')\n      plt.imshow(img)\n      plt.axis('off')\n\n  # tight_layout minimizes the overlap between 2 sub-plots\n  fig.suptitle(\"epoch: {}, step: {}\".format(epoch, step))\n  plt.savefig('image_at_epoch_{:04d}_step{:04d}.png'.format(epoch, step))\n  plt.show()\n\n    # Training loop. Display generated images each epoch\ndef VAE_training_loop(encoder, decoder, vae, training_dataset, validation_dataset):\n    epochs = 40\n    optimizer, loss_metric, mse_loss, bce_loss = model_configs()\n\n    random_vector_for_generation = tf.random.normal(shape=[16, LATENT_DIM])\n    generate_and_save_images(decoder, 0, 0, random_vector_for_generation)\n\n    for epoch in range(epochs):\n      print('Start of epoch %d' % (epoch,))\n\n      # Iterate over the batches of the dataset.\n      for step, x_batch_train in enumerate(training_dataset):\n        with tf.GradientTape() as tape:\n          reconstructed = vae(x_batch_train)\n\n          # compute reconstruction loss\n          flattened_inputs = tf.reshape(x_batch_train, shape=[-1])\n          flattened_outputs = tf.reshape(reconstructed, shape=[-1])\n          loss = bce_loss(flattened_inputs, flattened_outputs) * 4096\n\n          # add KLD regularization loss\n          loss += sum(vae.losses)   \n\n        grads = tape.gradient(loss, vae.trainable_weights)\n        optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n\n        loss_metric(loss)\n\n        if step % 10 == 0:\n          display.clear_output(wait=False)    \n          generate_and_save_images(decoder, epoch, step, random_vector_for_generation)\n\ndef AE_training_loop(encoder, decoder, vae, training_dataset, validation_dataset):\n    epochs = 40\n    optimizer, loss_metric, mse_loss, bce_loss = model_configs()\n\n    random_vector_for_generation = tf.random.normal(shape=[16, LATENT_DIM])\n    generate_and_save_images(decoder, 0, 0, random_vector_for_generation)\n\n    for epoch in range(epochs):\n      print('Start of epoch %d' % (epoch,))\n\n      # Iterate over the batches of the dataset.\n      for step, x_batch_train in enumerate(training_dataset):\n        with tf.GradientTape() as tape:    \n          reconstructed = vae(x_batch_train)\n\n          # compute reconstruction loss\n          flattened_inputs = tf.reshape(x_batch_train, shape=[-1])\n          flattened_outputs = tf.reshape(reconstructed, shape=[-1])\n          loss = mse_loss(flattened_inputs, flattened_outputs) * 4096\n\n\n        grads = tape.gradient(loss, vae.trainable_weights)\n        optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n\n        loss_metric(loss)\n\n        if step % 10 == 0:\n          display.clear_output(wait=False)    \n          generate_and_save_images(decoder, epoch, step, random_vector_for_generation)\n\n        \ndef plot_images(rows, cols, images, title):\n    '''Displays images in a grid.'''\n    grid = np.zeros(shape=(rows*64, cols*64, 3))\n    for row in range(rows):\n        for col in range(cols):\n            grid[row*64:(row+1)*64, col*64:(col+1)*64, :] = images[row*cols + col]\n\n    plt.figure(figsize=(12,12))       \n    plt.imshow(grid)\n    plt.title(title)\n    plt.show()\n    \ndef icon_GAN(filepath):\n    noisyLabels = False\n    optimizer = 'rmsprop' #vs adam #vs rmsprop\n    EPOCHS = 40\n    BATCH_SIZE = 256\n    channelThree = True\n    onesideSmooth = False #Overrules noisyLabels when true.\n    random_normal_dimensions = 32\n\n    #We know that using False, rms, 60, 128, false, false gave GOOD results.\n    #Noisy labels seems to make it mode collapse\n\n\n    def plot_results(images, n_cols=None):\n        '''visualizes fake images'''\n        display.clear_output(wait=False)  \n\n        n_cols = n_cols or len(images)\n        n_rows = (len(images) - 1) // n_cols + 1\n\n        if images.shape[-1] == 1:\n            images = np.squeeze(images, axis=-1)\n\n        plt.figure(figsize=(n_cols, n_rows))\n\n        for index, image in enumerate(images):\n            plt.subplot(n_rows, n_cols, index + 1)\n            plt.imshow(image, cmap=\"binary\")\n            plt.axis(\"off\")\n\n   \n    ## Get the training data\n\n    # create training batches\n    def create_and_map(filepath):\n        filename_dataset = tf.data.Dataset.list_files(filepath+\"/*.PNG\")\n        image_dataset = filename_dataset.map(map_image).shuffle(30000).batch(BATCH_SIZE)\n        \n        return image_dataset\n\n    # ## Build the generator\n    def gan_generator(channelThree, random_normal_dimensions):\n\n        generator = keras.models.Sequential()\n\n        generator.add(keras.layers.Dense(4 * 4 * 1024, input_shape=[random_normal_dimensions]))\n        generator.add(keras.layers.Reshape([4, 4, 1024]))\n        generator.add(keras.layers.BatchNormalization())\n        generator.add(keras.layers.LeakyReLU())\n\n        generator.add(keras.layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"SAME\", activation=\"selu\"))\n        generator.add(keras.layers.BatchNormalization())\n\n        generator.add(keras.layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"SAME\", activation=\"selu\"))\n        generator.add(keras.layers.BatchNormalization())\n\n        generator.add(keras.layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"SAME\", activation=\"selu\"))\n        generator.add(keras.layers.BatchNormalization())\n\n        if not channelThree:\n            generator.add(keras.layers.Conv2DTranspose(4, kernel_size=4, strides=2, padding=\"SAME\", activation=\"tanh\"))\n        else:\n            generator.add(keras.layers.Conv2DTranspose(3, kernel_size=4, strides=2, padding=\"SAME\", activation=\"tanh\"))\n\n        print(generator.output_shape)\n        print(generator.summary())\n        \n        return generator\n   \n    # ## Build the discriminator\n    def gan_discriminator(channelThree, optimizer):\n        discriminator = keras.models.Sequential()\n        if not channelThree:\n            discriminator.add(keras.layers.Conv2D(64, kernel_size=4, strides=2, padding=\"SAME\",\n                                activation=keras.layers.LeakyReLU(0.2),\n                                input_shape=[64, 64, 4]))\n        else:\n            discriminator.add(keras.layers.Conv2D(64, kernel_size=4, strides=2, padding=\"SAME\",\n                                activation=keras.layers.LeakyReLU(0.2),\n                                input_shape=[64, 64, 3]))\n\n        discriminator.add(keras.layers.Dropout(0.4))\n        discriminator.add(keras.layers.Conv2D(128, kernel_size=4, strides=2, padding=\"SAME\",\n                                activation=keras.layers.LeakyReLU(0.2)))\n\n        discriminator.add(keras.layers.Dropout(0.4))\n        discriminator.add(keras.layers.Conv2D(256, kernel_size=4, strides=2, padding=\"SAME\",\n                                activation=keras.layers.LeakyReLU(0.2)))\n        discriminator.add(keras.layers.Dropout(0.4))\n        discriminator.add(keras.layers.Conv2D(512, kernel_size=4, strides=2, padding=\"SAME\",\n                                activation=keras.layers.LeakyReLU(0.2)))\n        discriminator.add(keras.layers.Dropout(0.4))\n        discriminator.add(keras.layers.Flatten())\n        discriminator.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n\n        print(discriminator.summary())\n        \n        discriminator.compile(loss=\"binary_crossentropy\", optimizer=optimizer)\n        discriminator.trainable = False\n        \n        return discriminator\n\n    # ## Build and compile the GAN model\n    def create_and_compile_gan(generator, discriminator, optimizer):\n        gan = keras.models.Sequential([generator, discriminator])\n        gan.compile(loss=\"binary_crossentropy\", optimizer=optimizer)\n        \n        return gan\n\n    # ## Train the GAN\n   \n    # - real_batch_size: Get the batch size of the input batch (it's the zero-th dimension of the tensor)\n    # - noise: Generate the noise using `tf.random.normal`.  The shape is batch size x random_normal_dimension\n    # - fake images: Pass in the noise and produce fake images.\n    # - mixed_images: concatenate the fake images with the real images.\n    # - Set the axis to 0.\n    # - discriminator_labels: Set to `0.` for fake images and `1.` for real images.\n    # - Set the discriminator as trainable.\n    # - Use the discriminator's `train_on_batch()` method to train on the mixed images and the discriminator labels.\n    # \n    # \n    # Phase 2\n    # - noise: generate random normal values with dimensions batch_size x random_normal_dimensions\n    # - Generator_labels: Set to `1.` to mark the fake images as real\n    # - The generator will generate fake images that are labeled as real images and attempt to fool the discriminator.\n    # - Set the discriminator to NOT be trainable.\n    # - Train the GAN on the noise and the generator labels.\n\n   \n    def train_gan(gan, dataset, random_normal_dimensions, n_epochs=50):\n        \"\"\" Defines the two-phase training loop of the GAN\n        Args:\n          gan -- the GAN model which has the generator and discriminator\n          dataset -- the training set of real images\n          random_normal_dimensions -- dimensionality of the input to the generator\n          n_epochs -- number of epochs\n        \"\"\"\n\n        # get the two sub networks from the GAN model\n        generator, discriminator = gan.layers\n\n        for epoch in range(n_epochs):\n            print(\"Epoch {}/{}\".format(epoch + 1, n_epochs))       \n            for real_images in dataset:\n\n                # infer batch size from the current batch of real images\n                real_batch_size = real_images.shape[0]\n                # Train the discriminator - PHASE 1\n                # Create the noise\n                noise = tf.random.normal(shape=[real_batch_size, random_normal_dimensions])\n\n                # Use the noise to generate fake images\n                fake_images = generator(noise)\n\n                # Create a list by concatenating the fake images with the real ones\n                mixed_images = tf.concat([fake_images, real_images], axis=0)\n\n\n\n                # Create the labels for the discriminator\n                # Soft labels\n                if noisyLabels:\n                    discriminator_labels = tf.constant([[.15]] * real_batch_size + [[.9]] * real_batch_size)\n                else:\n                    discriminator_labels = tf.constant([[0]] * real_batch_size + [[1]] * real_batch_size)\n\n                if onesideSmooth:\n                    discriminator_labels = tf.constant([[0]] * real_batch_size + [[.9]] * real_batch_size)\n\n\n                # Ensure that the discriminator is trainable\n                discriminator.trainable = True\n\n                # Use train_on_batch to train the discriminator with the mixed images and the discriminator labels\n                discriminator.train_on_batch(mixed_images, discriminator_labels)\n\n                # Train the generator - PHASE 2\n                # create a batch of noise input to feed to the GAN\n                noise = tf.random.normal(shape=[real_batch_size, random_normal_dimensions])\n\n                # label all generated images to be \"real\"\n                generator_labels = tf.constant([[1.]]*real_batch_size)\n\n                # Freeze the discriminator\n                discriminator.trainable = False\n\n                # Train the GAN on the noise with the labels all set to be true\n                gan.train_on_batch(noise, generator_labels)\n\n\n            plot_results(fake_images, 16)                     \n            plt.show()\n\n        return fake_images\n\n    def collect_and_save_images(fake_images):\n        for i in range(len(fake_images)):\n            tf.keras.preprocessing.image.save_img(\"fake\"+str(i)+\".png\",fake_images[i])\n        \n    image_dataset = create_and_map(filepath)\n    generator = gan_generator(channelThree, random_normal_dimensions)\n    discriminator = gan_discriminator(channelThree, optimizer)\n    gan = create_and_compile_gan(generator, discriminator, optimizer)\n    fake_images = train_gan(gan, image_dataset, random_normal_dimensions, EPOCHS)\n    collect_and_save_images(fake_images)\n\n    \ndef main(generator_version, filepath):\n    if generator_version == 'VAE':\n        training_dataset, validation_dataset = get_data(filepath)\n        encoder, decoder, vae = get_VAE_models(input_shape=(64,64,3,), latent_dim=LATENT_DIM)\n        VAE_training_loop(encoder, decoder, vae, training_dataset, validation_dataset)\n        \n    elif generator_version == 'AE':\n        training_dataset, validation_dataset = get_data(filepath)\n        encoder, decoder, ae = get_AE_models(input_shape=(64,64,3,), latent_dim=LATENT_DIM)\n        AE_training_loop(encoder, decoder, ae, training_dataset, validation_dataset)\n    \n    elif generator_version == 'GAN':\n        icon_GAN(filepath)\n        \n    else:\n        print(\"Please select valid generator type\")\n        \nmain(\"GAN\", \"../input/icons2\")\n\n    \n    ","metadata":{"collapsed":false,"_kg_hide-input":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-03T21:52:36.301050Z","iopub.execute_input":"2021-12-03T21:52:36.301439Z","iopub.status.idle":"2021-12-03T21:54:37.815806Z","shell.execute_reply.started":"2021-12-03T21:52:36.301402Z","shell.execute_reply":"2021-12-03T21:54:37.814473Z"},"trusted":true},"execution_count":2,"outputs":[]}]}